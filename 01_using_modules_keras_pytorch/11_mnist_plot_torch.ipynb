{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ではpytorchでも実装していきましょう。pytorchでは自前で配列を用意する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "        self.l3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.a3 = nn.ReLU()\n",
    "        self.d3 = nn.Dropout(0.5)\n",
    "        self.l4 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.layers = [self.l1, self.a1, self.d1,\n",
    "                       self.l2, self.a2, self.d2,\n",
    "                       self.l3, self.a3, self.d3,\n",
    "                       self.l4]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に各出力を確率に落とし込むためにnn.sigmoidやnn.softmaxを挟む必要性がありそうに感じますが、実際には必要ありません。<br>\n",
    "今回、損失関数としてnn.CrossEntropyLoss()を使用しますが、この関数の中では実際には二つの関数が動いており、<br>\n",
    "nn.LogSoftmax(x)　　　ソフトマックス関数の対数を計算  $\\log(softmax({\\bf x})_i) = x_i - \\log \\sum_{j=1}^n \\exp\\{{x_j}\\}$<br>\n",
    "nn.NLLLoss(x, t)　　　負の対数尤度を計算　　　　　　　　　 $-\\{t_i(\\log(x_i))+(1-t_i)(\\log(1-x_i))\\} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(784, 200, 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join('~', '.torch', 'mnist')\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               lambda x: x.view(-1)])\n",
    "mnist_train = datasets.MNIST(root = root, \n",
    "                            download = True,\n",
    "                            train = True,\n",
    "                            transform = transform)\n",
    "mnist_test = datasets.MNIST(root = root, \n",
    "                            download = True,\n",
    "                            train = False,\n",
    "                            transform = transform)\n",
    "\n",
    "n_samples = len(mnist_train)\n",
    "n_train = int(n_samples * 0.8)\n",
    "n_val = n_samples - n_train\n",
    "\n",
    "mnist_train, mnist_val = random_split(mnist_train, [n_train, n_val])\n",
    "\n",
    "train_dataloader = DataLoader(mnist_train,\n",
    "                             batch_size=100,\n",
    "                             shuffle = True)\n",
    "val_dataloader   = DataLoader(mnist_val,\n",
    "                             batch_size=100,\n",
    "                             shuffle =False)\n",
    "test_dataloader  = DataLoader(mnist_test,\n",
    "                             batch_size=100,\n",
    "                             shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 2.29, acc: 0.134, val_loss: 2.27, val_acc: 0.298250\n",
      "epoch: 2, loss: 2.23, acc: 0.257, val_loss: 2.11, val_acc: 0.435667\n",
      "epoch: 3, loss: 1.88, acc: 0.401, val_loss: 1.35, val_acc: 0.624333\n",
      "epoch: 4, loss: 1.3, acc: 0.556, val_loss: 0.864, val_acc: 0.739833\n",
      "epoch: 5, loss: 0.999, acc: 0.666, val_loss: 0.669, val_acc: 0.801750\n",
      "epoch: 6, loss: 0.833, acc: 0.730, val_loss: 0.553, val_acc: 0.839750\n",
      "epoch: 7, loss: 0.722, acc: 0.772, val_loss: 0.481, val_acc: 0.858833\n",
      "epoch: 8, loss: 0.647, acc: 0.800, val_loss: 0.431, val_acc: 0.878667\n",
      "epoch: 9, loss: 0.589, acc: 0.822, val_loss: 0.388, val_acc: 0.888583\n",
      "epoch: 10, loss: 0.547, acc: 0.836, val_loss: 0.359, val_acc: 0.898250\n",
      "epoch: 11, loss: 0.506, acc: 0.851, val_loss: 0.335, val_acc: 0.902250\n",
      "epoch: 12, loss: 0.481, acc: 0.860, val_loss: 0.317, val_acc: 0.908750\n",
      "epoch: 13, loss: 0.453, acc: 0.868, val_loss: 0.296, val_acc: 0.916000\n",
      "epoch: 14, loss: 0.429, acc: 0.877, val_loss: 0.282, val_acc: 0.918583\n",
      "epoch: 15, loss: 0.411, acc: 0.882, val_loss: 0.266, val_acc: 0.923083\n",
      "epoch: 16, loss: 0.382, acc: 0.891, val_loss: 0.255, val_acc: 0.926000\n",
      "epoch: 17, loss: 0.374, acc: 0.891, val_loss: 0.242, val_acc: 0.929833\n",
      "epoch: 18, loss: 0.355, acc: 0.897, val_loss: 0.231, val_acc: 0.933583\n",
      "epoch: 19, loss: 0.345, acc: 0.902, val_loss: 0.223, val_acc: 0.935333\n",
      "epoch: 20, loss: 0.333, acc: 0.905, val_loss: 0.214, val_acc: 0.939000\n",
      "epoch: 21, loss: 0.322, acc: 0.909, val_loss: 0.207, val_acc: 0.940000\n",
      "epoch: 22, loss: 0.313, acc: 0.911, val_loss: 0.199, val_acc: 0.942667\n",
      "epoch: 23, loss: 0.301, acc: 0.915, val_loss: 0.192, val_acc: 0.944833\n",
      "epoch: 24, loss: 0.293, acc: 0.917, val_loss: 0.188, val_acc: 0.945750\n",
      "epoch: 25, loss: 0.284, acc: 0.920, val_loss: 0.184, val_acc: 0.947833\n",
      "epoch: 26, loss: 0.278, acc: 0.921, val_loss: 0.177, val_acc: 0.947250\n",
      "epoch: 27, loss: 0.269, acc: 0.924, val_loss: 0.174, val_acc: 0.949667\n",
      "epoch: 28, loss: 0.26, acc: 0.925, val_loss: 0.168, val_acc: 0.950333\n",
      "epoch: 29, loss: 0.254, acc: 0.929, val_loss: 0.164, val_acc: 0.951583\n",
      "epoch: 30, loss: 0.246, acc: 0.930, val_loss: 0.161, val_acc: 0.953000\n",
      "epoch: 31, loss: 0.239, acc: 0.932, val_loss: 0.156, val_acc: 0.954500\n",
      "epoch: 32, loss: 0.235, acc: 0.933, val_loss: 0.154, val_acc: 0.954333\n",
      "epoch: 33, loss: 0.233, acc: 0.934, val_loss: 0.153, val_acc: 0.954333\n",
      "epoch: 34, loss: 0.227, acc: 0.935, val_loss: 0.149, val_acc: 0.956583\n",
      "epoch: 35, loss: 0.224, acc: 0.937, val_loss: 0.148, val_acc: 0.957000\n",
      "epoch: 36, loss: 0.223, acc: 0.937, val_loss: 0.145, val_acc: 0.957583\n",
      "epoch: 37, loss: 0.217, acc: 0.938, val_loss: 0.14, val_acc: 0.958333\n",
      "epoch: 38, loss: 0.208, acc: 0.941, val_loss: 0.14, val_acc: 0.959667\n",
      "epoch: 39, loss: 0.211, acc: 0.940, val_loss: 0.137, val_acc: 0.960083\n",
      "epoch: 40, loss: 0.202, acc: 0.943, val_loss: 0.136, val_acc: 0.959917\n",
      "epoch: 41, loss: 0.201, acc: 0.943, val_loss: 0.133, val_acc: 0.961417\n",
      "epoch: 42, loss: 0.195, acc: 0.946, val_loss: 0.132, val_acc: 0.961917\n",
      "epoch: 43, loss: 0.192, acc: 0.946, val_loss: 0.13, val_acc: 0.962583\n",
      "epoch: 44, loss: 0.19, acc: 0.947, val_loss: 0.129, val_acc: 0.962167\n",
      "epoch: 45, loss: 0.187, acc: 0.948, val_loss: 0.126, val_acc: 0.963083\n",
      "epoch: 46, loss: 0.188, acc: 0.948, val_loss: 0.124, val_acc: 0.963833\n",
      "epoch: 47, loss: 0.18, acc: 0.949, val_loss: 0.125, val_acc: 0.964917\n",
      "epoch: 48, loss: 0.179, acc: 0.949, val_loss: 0.123, val_acc: 0.964500\n",
      "epoch: 49, loss: 0.177, acc: 0.950, val_loss: 0.121, val_acc: 0.965500\n",
      "epoch: 50, loss: 0.17, acc: 0.951, val_loss: 0.122, val_acc: 0.964750\n",
      "epoch: 51, loss: 0.17, acc: 0.951, val_loss: 0.12, val_acc: 0.965083\n",
      "epoch: 52, loss: 0.17, acc: 0.952, val_loss: 0.118, val_acc: 0.964917\n",
      "epoch: 53, loss: 0.169, acc: 0.952, val_loss: 0.118, val_acc: 0.965167\n",
      "epoch: 54, loss: 0.166, acc: 0.953, val_loss: 0.117, val_acc: 0.965333\n",
      "epoch: 55, loss: 0.161, acc: 0.954, val_loss: 0.117, val_acc: 0.966333\n",
      "epoch: 56, loss: 0.161, acc: 0.954, val_loss: 0.116, val_acc: 0.966833\n",
      "epoch: 57, loss: 0.162, acc: 0.953, val_loss: 0.113, val_acc: 0.967833\n",
      "epoch: 58, loss: 0.158, acc: 0.954, val_loss: 0.112, val_acc: 0.967917\n",
      "epoch: 59, loss: 0.153, acc: 0.958, val_loss: 0.111, val_acc: 0.968333\n",
      "epoch: 60, loss: 0.153, acc: 0.958, val_loss: 0.111, val_acc: 0.967250\n",
      "epoch: 61, loss: 0.154, acc: 0.956, val_loss: 0.109, val_acc: 0.968250\n",
      "epoch: 62, loss: 0.15, acc: 0.958, val_loss: 0.109, val_acc: 0.968500\n",
      "epoch: 63, loss: 0.148, acc: 0.958, val_loss: 0.108, val_acc: 0.968833\n",
      "epoch: 64, loss: 0.144, acc: 0.958, val_loss: 0.108, val_acc: 0.969167\n",
      "epoch: 65, loss: 0.145, acc: 0.958, val_loss: 0.109, val_acc: 0.969833\n",
      "epoch: 66, loss: 0.143, acc: 0.959, val_loss: 0.106, val_acc: 0.969417\n",
      "epoch: 67, loss: 0.142, acc: 0.959, val_loss: 0.105, val_acc: 0.969250\n",
      "epoch: 68, loss: 0.142, acc: 0.960, val_loss: 0.107, val_acc: 0.969833\n",
      "epoch: 69, loss: 0.138, acc: 0.959, val_loss: 0.105, val_acc: 0.969500\n",
      "epoch: 70, loss: 0.136, acc: 0.960, val_loss: 0.102, val_acc: 0.971167\n",
      "epoch: 71, loss: 0.136, acc: 0.961, val_loss: 0.104, val_acc: 0.970500\n",
      "epoch: 72, loss: 0.134, acc: 0.962, val_loss: 0.105, val_acc: 0.969667\n",
      "epoch: 73, loss: 0.137, acc: 0.961, val_loss: 0.102, val_acc: 0.971333\n",
      "epoch: 74, loss: 0.133, acc: 0.961, val_loss: 0.102, val_acc: 0.970667\n",
      "epoch: 75, loss: 0.134, acc: 0.962, val_loss: 0.102, val_acc: 0.971167\n",
      "epoch: 76, loss: 0.126, acc: 0.965, val_loss: 0.102, val_acc: 0.971250\n",
      "epoch: 77, loss: 0.128, acc: 0.963, val_loss: 0.1, val_acc: 0.972167\n",
      "epoch: 78, loss: 0.128, acc: 0.963, val_loss: 0.0994, val_acc: 0.971583\n",
      "epoch: 79, loss: 0.123, acc: 0.963, val_loss: 0.0993, val_acc: 0.971583\n",
      "epoch: 80, loss: 0.126, acc: 0.964, val_loss: 0.0992, val_acc: 0.971667\n",
      "epoch: 81, loss: 0.125, acc: 0.964, val_loss: 0.0988, val_acc: 0.972167\n",
      "epoch: 82, loss: 0.12, acc: 0.965, val_loss: 0.0992, val_acc: 0.972167\n",
      "epoch: 83, loss: 0.12, acc: 0.965, val_loss: 0.0983, val_acc: 0.972417\n",
      "epoch: 84, loss: 0.121, acc: 0.965, val_loss: 0.098, val_acc: 0.972583\n",
      "epoch: 85, loss: 0.117, acc: 0.966, val_loss: 0.0988, val_acc: 0.972250\n",
      "epoch: 86, loss: 0.118, acc: 0.966, val_loss: 0.099, val_acc: 0.972167\n",
      "epoch: 87, loss: 0.117, acc: 0.966, val_loss: 0.0979, val_acc: 0.973167\n",
      "epoch: 88, loss: 0.115, acc: 0.967, val_loss: 0.0977, val_acc: 0.972750\n",
      "epoch: 89, loss: 0.113, acc: 0.967, val_loss: 0.0971, val_acc: 0.972083\n",
      "epoch: 90, loss: 0.116, acc: 0.967, val_loss: 0.0952, val_acc: 0.972500\n",
      "epoch: 91, loss: 0.115, acc: 0.967, val_loss: 0.095, val_acc: 0.973917\n",
      "epoch: 92, loss: 0.113, acc: 0.967, val_loss: 0.0972, val_acc: 0.973333\n",
      "epoch: 93, loss: 0.111, acc: 0.968, val_loss: 0.0967, val_acc: 0.973333\n",
      "epoch: 94, loss: 0.11, acc: 0.968, val_loss: 0.0954, val_acc: 0.973333\n",
      "epoch: 95, loss: 0.108, acc: 0.969, val_loss: 0.0956, val_acc: 0.974333\n",
      "epoch: 96, loss: 0.106, acc: 0.969, val_loss: 0.0956, val_acc: 0.974417\n",
      "epoch: 97, loss: 0.108, acc: 0.968, val_loss: 0.0942, val_acc: 0.973500\n",
      "epoch: 98, loss: 0.107, acc: 0.969, val_loss: 0.0942, val_acc: 0.973583\n",
      "epoch: 99, loss: 0.104, acc: 0.970, val_loss: 0.0941, val_acc: 0.974583\n",
      "epoch: 100, loss: 0.106, acc: 0.970, val_loss: 0.0934, val_acc: 0.974917\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizers.SGD(model.parameters(), lr=0.01)\n",
    "def compute_loss(t, y):\n",
    "    return criterion(y, t)\n",
    "def train_step(x, t):\n",
    "    model.train()\n",
    "    preds = model(x)\n",
    "    loss = compute_loss(t, preds)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, preds\n",
    "def val_step(x, t):\n",
    "    model.eval()\n",
    "    preds = model(x)\n",
    "    loss = criterion(preds, t)\n",
    "    return loss, preds\n",
    "\n",
    "epochs = 100\n",
    "hist = {'val_loss': [], 'val_accuracy': []}\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.\n",
    "    train_acc  = 0.\n",
    "    val_loss   = 0.\n",
    "    val_acc    = 0.\n",
    "    for (x, t) in train_dataloader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        loss, preds = train_step(x, t)\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy_score(t.tolist(), preds.argmax(dim=-1).tolist())\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc  /= len(train_dataloader)\n",
    "    \n",
    "    for (x, t) in val_dataloader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        loss, preds = val_step(x, t)\n",
    "        val_loss += loss.item()\n",
    "        val_acc += accuracy_score(t.tolist(), preds.argmax(dim=-1).tolist())\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_acc  /= len(val_dataloader)\n",
    "    hist['val_loss'].append(val_loss)\n",
    "    hist['val_accuracy'].append(val_acc)\n",
    "    print('epoch: {}, loss: {:.3}, acc: {:.3f}, val_loss: {:.3}, val_acc: {:.3f}'.format(\n",
    "                epoch+1,\n",
    "                train_loss,\n",
    "                train_acc,\n",
    "                val_loss,\n",
    "                val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss:0.154, test_acc:0.955\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_loss = hist['val_loss']\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(val_loss)), val_loss, color='black', linewidth=1)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "def test_step(x, t):\n",
    "    return val_step(x, t)\n",
    "\n",
    "test_loss = 0.\n",
    "test_acc = 0.\n",
    "for (x, t) in test_dataloader:\n",
    "    x, t = x.to(device), t.to(device)\n",
    "    loss, preds = train_step(x, t)\n",
    "    test_loss += loss.item()\n",
    "    test_acc += accuracy_score(t.tolist(), preds.argmax(dim=-1).tolist())\n",
    "test_loss /= len(test_dataloader)\n",
    "test_acc /= len(test_dataloader)\n",
    "print('test_loss:{:.3f}, test_acc:{:.3f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
